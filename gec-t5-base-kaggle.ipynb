{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6986595,"sourceType":"datasetVersion","datasetId":4015274}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-22T06:12:07.796009Z","iopub.execute_input":"2023-12-22T06:12:07.796464Z","iopub.status.idle":"2023-12-22T06:12:08.197745Z","shell.execute_reply.started":"2023-12-22T06:12:07.796429Z","shell.execute_reply":"2023-12-22T06:12:08.196735Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/jfleg-dataset/eval.csv\n/kaggle/input/jfleg-dataset/train.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoTokenizer\nfrom transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\nfrom transformers import DataCollatorForSeq2Seq\nimport datasets\nimport pandas as pd\n","metadata":{"execution":{"iopub.status.busy":"2023-12-22T06:12:08.199811Z","iopub.execute_input":"2023-12-22T06:12:08.200218Z","iopub.status.idle":"2023-12-22T06:12:25.795312Z","shell.execute_reply.started":"2023-12-22T06:12:08.200192Z","shell.execute_reply":"2023-12-22T06:12:25.793980Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"checkpoint = \"t5-base\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)","metadata":{"execution":{"iopub.status.busy":"2023-12-22T06:12:25.796746Z","iopub.execute_input":"2023-12-22T06:12:25.797522Z","iopub.status.idle":"2023-12-22T06:12:32.132172Z","shell.execute_reply.started":"2023-12-22T06:12:25.797492Z","shell.execute_reply":"2023-12-22T06:12:32.131280Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6714a5248d7472e9f62561ef2a258cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87e04583d17e44a5842dd0a76ec6aaee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65a77c80975148e8a41245c7b6934449"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py:160: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\nFor now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2dcaa4b8e2841088eb5e3002f1572d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1afb83575b2849c6a917687c690bf5b4"}},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\n\ntrain_df = pd.read_csv(\"/kaggle/input/jfleg-dataset/train.csv\")\ntest_def = pd.read_csv(\"/kaggle/input/jfleg-dataset/eval.csv\")\n\ncombined_df = pd.concat([train_df, test_def], ignore_index=True)\ncombined_df.to_csv(\"/kaggle/working/combined.csv\", index=False)\ncombined_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-22T06:12:32.133378Z","iopub.execute_input":"2023-12-22T06:12:32.133655Z","iopub.status.idle":"2023-12-22T06:12:32.277210Z","shell.execute_reply.started":"2023-12-22T06:12:32.133631Z","shell.execute_reply":"2023-12-22T06:12:32.276126Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                               input  \\\n0  So I think we can not live if old people could...   \n1  So I think we can not live if old people could...   \n2  So I think we can not live if old people could...   \n3  So I think we can not live if old people could...   \n4                                  For not use car.    \n\n                                              target  \n0  So I think we would not be alive if our ancest...  \n1  So I think we could not live if older people d...  \n2  So I think we can not live if old people could...  \n3  So I think we can not live if old people can n...  \n4                           Not for use with a car.   ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>input</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>So I think we can not live if old people could...</td>\n      <td>So I think we would not be alive if our ancest...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>So I think we can not live if old people could...</td>\n      <td>So I think we could not live if older people d...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>So I think we can not live if old people could...</td>\n      <td>So I think we can not live if old people could...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>So I think we can not live if old people could...</td>\n      <td>So I think we can not live if old people can n...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>For not use car.</td>\n      <td>Not for use with a car.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import json\n# Convert combined_df to json format and save it.\ndata = {'data': []}\n\nfor i in range(len(combined_df)):\n    json_data = {}\n    json_data = {\n        \"input\": \"grammar: \" + combined_df['input'][i],\n        \"target\": combined_df['target'][i]\n    }   \n    data['data'].append(json_data)\nwith open('/kaggle/working/combined.json', 'w') as f:\n    json.dump(data, f, indent=4, separators=(',', ': ') )","metadata":{"execution":{"iopub.status.busy":"2023-12-22T06:12:32.280438Z","iopub.execute_input":"2023-12-22T06:12:32.280821Z","iopub.status.idle":"2023-12-22T06:12:32.485084Z","shell.execute_reply.started":"2023-12-22T06:12:32.280782Z","shell.execute_reply":"2023-12-22T06:12:32.484106Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\ndataset = load_dataset('json', data_files='/kaggle/working/combined.json',field=\"data\")","metadata":{"execution":{"iopub.status.busy":"2023-12-22T06:12:32.486364Z","iopub.execute_input":"2023-12-22T06:12:32.486745Z","iopub.status.idle":"2023-12-22T06:12:32.720600Z","shell.execute_reply.started":"2023-12-22T06:12:32.486711Z","shell.execute_reply":"2023-12-22T06:12:32.719640Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-ea24147818b0cc8f/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"215ceaea765e47d1ae20d45723d1be42"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"148411fd30d942289322d4e9b740da7e"}},"metadata":{}},{"name":"stdout","text":"Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-ea24147818b0cc8f/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ec09a4aa4af4bc9b9f4da57aeb8d7f6"}},"metadata":{}}]},{"cell_type":"code","source":"dataset = dataset['train'].train_test_split(test_size=0.2)\ndataset","metadata":{"execution":{"iopub.status.busy":"2023-12-22T06:12:32.721864Z","iopub.execute_input":"2023-12-22T06:12:32.722159Z","iopub.status.idle":"2023-12-22T06:12:32.741684Z","shell.execute_reply.started":"2023-12-22T06:12:32.722132Z","shell.execute_reply":"2023-12-22T06:12:32.740533Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['input', 'target'],\n        num_rows: 4803\n    })\n    test: Dataset({\n        features: ['input', 'target'],\n        num_rows: 1201\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"dataset['train'][0]","metadata":{"execution":{"iopub.status.busy":"2023-12-22T06:12:32.743193Z","iopub.execute_input":"2023-12-22T06:12:32.743568Z","iopub.status.idle":"2023-12-22T06:12:32.750509Z","shell.execute_reply.started":"2023-12-22T06:12:32.743539Z","shell.execute_reply":"2023-12-22T06:12:32.749513Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"{'input': 'grammar: Or, \" Is this the right thing to do? \" ',\n 'target': 'Or, \" Is this the right thing to do? \" '}"},"metadata":{}}]},{"cell_type":"code","source":"def preprocess_func(examples):\n    inputs = [source for source in examples['input']]\n    targets = [target for target in examples['target']]\n    model_inputs = tokenizer(inputs, text_target=targets, max_length=1024, truncation=True)\n    return model_inputs","metadata":{"execution":{"iopub.status.busy":"2023-12-22T06:12:32.751884Z","iopub.execute_input":"2023-12-22T06:12:32.752230Z","iopub.status.idle":"2023-12-22T06:12:32.760045Z","shell.execute_reply.started":"2023-12-22T06:12:32.752198Z","shell.execute_reply":"2023-12-22T06:12:32.759234Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"tokenized_data = dataset.map(preprocess_func, batched=True)","metadata":{"execution":{"iopub.status.busy":"2023-12-22T06:12:32.761221Z","iopub.execute_input":"2023-12-22T06:12:32.761588Z","iopub.status.idle":"2023-12-22T06:12:33.531996Z","shell.execute_reply.started":"2023-12-22T06:12:32.761552Z","shell.execute_reply":"2023-12-22T06:12:33.530991Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6aeb5454a80946a78559d5a55236315b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cfdd331aa56e4eaca6803cfc6c23da7e"}},"metadata":{}}]},{"cell_type":"code","source":"tokenized_data","metadata":{"execution":{"iopub.status.busy":"2023-12-22T06:12:33.533656Z","iopub.execute_input":"2023-12-22T06:12:33.534053Z","iopub.status.idle":"2023-12-22T06:12:33.540793Z","shell.execute_reply.started":"2023-12-22T06:12:33.534016Z","shell.execute_reply":"2023-12-22T06:12:33.539655Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['input', 'target', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 4803\n    })\n    test: Dataset({\n        features: ['input', 'target', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 1201\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"!pip install evaluate sacrebleu","metadata":{"execution":{"iopub.status.busy":"2023-12-22T06:12:33.542051Z","iopub.execute_input":"2023-12-22T06:12:33.542392Z","iopub.status.idle":"2023-12-22T06:12:52.531009Z","shell.execute_reply.started":"2023-12-22T06:12:33.542364Z","shell.execute_reply":"2023-12-22T06:12:52.529691Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting evaluate\n  Obtaining dependency information for evaluate from https://files.pythonhosted.org/packages/70/63/7644a1eb7b0297e585a6adec98ed9e575309bb973c33b394dae66bc35c69/evaluate-0.4.1-py3-none-any.whl.metadata\n  Downloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\nCollecting sacrebleu\n  Obtaining dependency information for sacrebleu from https://files.pythonhosted.org/packages/de/ea/025db0a39337b63d4728a900d262c39c3029b0fe76a9876ce6297b1aa6a0/sacrebleu-2.4.0-py3-none-any.whl.metadata\n  Downloading sacrebleu-2.4.0-py3-none-any.whl.metadata (57 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.4/57.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.24.3)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.0.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.15)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2023.12.2)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.19.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.18.0)\nCollecting portalocker (from sacrebleu)\n  Obtaining dependency information for portalocker from https://files.pythonhosted.org/packages/17/9e/87671efcca80ba6203811540ed1f9c0462c1609d2281d7b7f53cef05da3d/portalocker-2.8.2-py3-none-any.whl.metadata\n  Downloading portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (2023.8.8)\nRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (4.9.3)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (11.0.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.8.5)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2023.11.17)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sacrebleu-2.4.0-py3-none-any.whl (106 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading portalocker-2.8.2-py3-none-any.whl (17 kB)\nInstalling collected packages: portalocker, sacrebleu, evaluate\nSuccessfully installed evaluate-0.4.1 portalocker-2.8.2 sacrebleu-2.4.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import evaluate\nmetric = evaluate.load(\"sacrebleu\")\n\n# Then create a function that passes your predictions and labels to compute to calculate the SacreBLEU score:\nimport numpy as np\n\ndef postprocess_text(preds, labels):\n    preds = [pred.strip() for pred in preds]\n    labels = [[label.strip()] for label in labels]\n    return preds, labels\n\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n    if isinstance(preds, tuple):\n        preds = preds[0]\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True, max_new_tokens=1024)\n#     print(f\"Decoded preds: {decoded_preds} \\n Len = {len(decoded_preds)}\")\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n\n    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n    result = {\"bleu\": result[\"score\"]}\n\n    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n    result[\"gen_len\"] = np.mean(prediction_lens)\n    result = {k: round(v, 4) for k, v in result.items()}\n    return result","metadata":{"execution":{"iopub.status.busy":"2023-12-22T06:12:52.532841Z","iopub.execute_input":"2023-12-22T06:12:52.533226Z","iopub.status.idle":"2023-12-22T06:12:55.093212Z","shell.execute_reply.started":"2023-12-22T06:12:52.533192Z","shell.execute_reply":"2023-12-22T06:12:55.092242Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/8.15k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d2a720859db4e6d89c5d8deb43445da"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)","metadata":{"execution":{"iopub.status.busy":"2023-12-22T06:12:55.097644Z","iopub.execute_input":"2023-12-22T06:12:55.098887Z","iopub.status.idle":"2023-12-22T06:12:55.103716Z","shell.execute_reply.started":"2023-12-22T06:12:55.098852Z","shell.execute_reply":"2023-12-22T06:12:55.102534Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"trainer-state\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    weight_decay=0.01,\n    save_total_limit=2,\n    num_train_epochs=40,\n    predict_with_generate=True,\n    fp16=True,\n    push_to_hub=False,\n    logging_dir=\"logging\",\n    logging_strategy=\"epoch\",\n#     generation_max_length=1024,\n#     max_new_tokens=1024,  \n)\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_data[\"train\"],\n    eval_dataset=tokenized_data[\"test\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-22T06:12:55.105098Z","iopub.execute_input":"2023-12-22T06:12:55.105478Z","iopub.status.idle":"2023-12-22T06:13:01.743563Z","shell.execute_reply.started":"2023-12-22T06:12:55.105443Z","shell.execute_reply":"2023-12-22T06:13:01.742645Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-12-22T06:13:01.744811Z","iopub.execute_input":"2023-12-22T06:13:01.745119Z","iopub.status.idle":"2023-12-22T07:40:37.668726Z","shell.execute_reply.started":"2023-12-22T06:13:01.745089Z","shell.execute_reply":"2023-12-22T07:40:37.667491Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231222_061409-4svijeuk</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/turiabu/huggingface/runs/4svijeuk' target=\"_blank\">dutiful-hill-25</a></strong> to <a href='https://wandb.ai/turiabu/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/turiabu/huggingface' target=\"_blank\">https://wandb.ai/turiabu/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/turiabu/huggingface/runs/4svijeuk' target=\"_blank\">https://wandb.ai/turiabu/huggingface/runs/4svijeuk</a>"},"metadata":{}},{"name":"stderr","text":"You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='12040' max='12040' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [12040/12040 1:25:54, Epoch 40/40]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Bleu</th>\n      <th>Gen Len</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.712100</td>\n      <td>0.501570</td>\n      <td>50.857200</td>\n      <td>16.410500</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.553600</td>\n      <td>0.467730</td>\n      <td>51.967400</td>\n      <td>16.394700</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.503900</td>\n      <td>0.446720</td>\n      <td>52.628600</td>\n      <td>16.354700</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.466600</td>\n      <td>0.435286</td>\n      <td>52.820100</td>\n      <td>16.356400</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.435500</td>\n      <td>0.423996</td>\n      <td>53.141300</td>\n      <td>16.333100</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.406700</td>\n      <td>0.419589</td>\n      <td>53.291100</td>\n      <td>16.326400</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.390200</td>\n      <td>0.416209</td>\n      <td>53.657200</td>\n      <td>16.318900</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.368600</td>\n      <td>0.416930</td>\n      <td>53.667000</td>\n      <td>16.302200</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.353300</td>\n      <td>0.417876</td>\n      <td>53.542900</td>\n      <td>16.308900</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.342100</td>\n      <td>0.420589</td>\n      <td>53.400700</td>\n      <td>16.298900</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.327500</td>\n      <td>0.423007</td>\n      <td>53.567600</td>\n      <td>16.312200</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.317400</td>\n      <td>0.425065</td>\n      <td>53.617900</td>\n      <td>16.298900</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.308400</td>\n      <td>0.429220</td>\n      <td>53.530300</td>\n      <td>16.315600</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.300300</td>\n      <td>0.433483</td>\n      <td>53.598800</td>\n      <td>16.311400</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.292300</td>\n      <td>0.434729</td>\n      <td>53.516800</td>\n      <td>16.319700</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>0.282200</td>\n      <td>0.437779</td>\n      <td>53.501300</td>\n      <td>16.338100</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>0.277100</td>\n      <td>0.442073</td>\n      <td>53.512500</td>\n      <td>16.336400</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>0.272900</td>\n      <td>0.445108</td>\n      <td>53.416800</td>\n      <td>16.347200</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>0.265200</td>\n      <td>0.448587</td>\n      <td>53.169100</td>\n      <td>16.337200</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.257900</td>\n      <td>0.452680</td>\n      <td>53.172700</td>\n      <td>16.343000</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>0.257700</td>\n      <td>0.454187</td>\n      <td>53.107500</td>\n      <td>16.343000</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>0.253400</td>\n      <td>0.456963</td>\n      <td>53.044700</td>\n      <td>16.338100</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>0.248500</td>\n      <td>0.459907</td>\n      <td>52.952400</td>\n      <td>16.328900</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>0.243200</td>\n      <td>0.464167</td>\n      <td>52.831400</td>\n      <td>16.332200</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>0.244400</td>\n      <td>0.467642</td>\n      <td>52.874200</td>\n      <td>16.333900</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>0.238100</td>\n      <td>0.469654</td>\n      <td>52.425700</td>\n      <td>16.330600</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>0.233500</td>\n      <td>0.475493</td>\n      <td>52.541400</td>\n      <td>16.320600</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>0.235000</td>\n      <td>0.474475</td>\n      <td>52.524800</td>\n      <td>16.326400</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>0.231200</td>\n      <td>0.478216</td>\n      <td>52.496600</td>\n      <td>16.303100</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.228200</td>\n      <td>0.480261</td>\n      <td>52.454000</td>\n      <td>16.318900</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>0.225000</td>\n      <td>0.481951</td>\n      <td>52.413600</td>\n      <td>16.316400</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>0.224100</td>\n      <td>0.484712</td>\n      <td>52.398500</td>\n      <td>16.324700</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>0.223100</td>\n      <td>0.485391</td>\n      <td>52.402100</td>\n      <td>16.310600</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>0.221100</td>\n      <td>0.487913</td>\n      <td>52.337700</td>\n      <td>16.311400</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>0.220000</td>\n      <td>0.489047</td>\n      <td>52.251600</td>\n      <td>16.302200</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>0.219500</td>\n      <td>0.488990</td>\n      <td>52.194900</td>\n      <td>16.310600</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>0.217600</td>\n      <td>0.490085</td>\n      <td>52.245600</td>\n      <td>16.310600</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>0.219700</td>\n      <td>0.490049</td>\n      <td>52.157000</td>\n      <td>16.302200</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>0.218300</td>\n      <td>0.490306</td>\n      <td>52.178400</td>\n      <td>16.302200</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.216900</td>\n      <td>0.490585</td>\n      <td>52.185100</td>\n      <td>16.303900</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=12040, training_loss=0.30130216909009355, metrics={'train_runtime': 5255.5621, 'train_samples_per_second': 36.556, 'train_steps_per_second': 2.291, 'total_flos': 1.172250361930752e+16, 'train_loss': 0.30130216909009355, 'epoch': 40.0})"},"metadata":{}}]},{"cell_type":"code","source":"trainer.save_model(\"gec-t5-base\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# zip model folder to download\n!zip -r trained_model1.zip /kaggle/working/gec-t5-base","metadata":{"execution":{"iopub.status.busy":"2023-12-22T07:51:52.628845Z","iopub.execute_input":"2023-12-22T07:51:52.629287Z","iopub.status.idle":"2023-12-22T07:52:48.002840Z","shell.execute_reply.started":"2023-12-22T07:51:52.629236Z","shell.execute_reply":"2023-12-22T07:52:48.001625Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"updating: kaggle/working/gec-t5-base/ (stored 0%)\nupdating: kaggle/working/gec-t5-base/config.json (deflated 63%)\nupdating: kaggle/working/gec-t5-base/generation_config.json (deflated 29%)\nupdating: kaggle/working/gec-t5-base/tokenizer.json (deflated 74%)\nupdating: kaggle/working/gec-t5-base/training_args.bin (deflated 49%)\nupdating: kaggle/working/gec-t5-base/spiece.model (deflated 48%)\nupdating: kaggle/working/gec-t5-base/tokenizer_config.json (deflated 95%)\nupdating: kaggle/working/gec-t5-base/model.safetensors (deflated 8%)\nupdating: kaggle/working/gec-t5-base/special_tokens_map.json (deflated 86%)\n","output_type":"stream"}]},{"cell_type":"code","source":"# zip model folder to download\n!zip -r fine-tuned.zip /kaggle/working/gec-t5-base","metadata":{"execution":{"iopub.status.busy":"2023-12-22T07:53:50.571623Z","iopub.execute_input":"2023-12-22T07:53:50.572616Z","iopub.status.idle":"2023-12-22T07:54:45.914283Z","shell.execute_reply.started":"2023-12-22T07:53:50.572580Z","shell.execute_reply":"2023-12-22T07:54:45.913078Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"  adding: kaggle/working/gec-t5-base/ (stored 0%)\n  adding: kaggle/working/gec-t5-base/config.json (deflated 63%)\n  adding: kaggle/working/gec-t5-base/generation_config.json (deflated 29%)\n  adding: kaggle/working/gec-t5-base/tokenizer.json (deflated 74%)\n  adding: kaggle/working/gec-t5-base/training_args.bin (deflated 49%)\n  adding: kaggle/working/gec-t5-base/spiece.model (deflated 48%)\n  adding: kaggle/working/gec-t5-base/tokenizer_config.json (deflated 95%)\n  adding: kaggle/working/gec-t5-base/model.safetensors (deflated 8%)\n  adding: kaggle/working/gec-t5-base/special_tokens_map.json (deflated 86%)\n","output_type":"stream"}]},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink('/kaggle/working/fine-tuned.zip')","metadata":{"execution":{"iopub.status.busy":"2023-12-22T07:56:16.940310Z","iopub.execute_input":"2023-12-22T07:56:16.940744Z","iopub.status.idle":"2023-12-22T07:56:16.952398Z","shell.execute_reply.started":"2023-12-22T07:56:16.940712Z","shell.execute_reply":"2023-12-22T07:56:16.950723Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/fine-tuned.zip","text/html":"<a href='/kaggle/working/fine-tuned.zip' target='_blank'>/kaggle/working/fine-tuned.zip</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"# zip trainer-state checkpoint folder to download\n!zip  trainer_state.zip /kaggle/working/trainer-state/checkpoint-12000","metadata":{"execution":{"iopub.status.busy":"2023-12-22T08:04:46.802205Z","iopub.execute_input":"2023-12-22T08:04:46.803562Z","iopub.status.idle":"2023-12-22T08:04:47.918719Z","shell.execute_reply.started":"2023-12-22T08:04:46.803519Z","shell.execute_reply":"2023-12-22T08:04:47.917073Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"updating: kaggle/working/trainer-state/checkpoint-12000/ (stored 0%)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Inference","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline\n\ncorrector_model = (\"text2text-generation\", model=\"/kaggle/working/gec-t5-base\")\ntext = \"Yesterday I go to a supermrket to buy groceries.\"\ncorrection = corrector(text)\ncorrection[0]['generated_text']","metadata":{"execution":{"iopub.status.busy":"2023-12-22T08:08:00.453516Z","iopub.execute_input":"2023-12-22T08:08:00.453943Z","iopub.status.idle":"2023-12-22T08:08:01.355778Z","shell.execute_reply.started":"2023-12-22T08:08:00.453913Z","shell.execute_reply":"2023-12-22T08:08:01.354522Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"/kaggle/working/gec-t5-base\", model_max_length=1024)\ncorrector = AutoModelForSeq2SeqLM.from_pretrained(\"/kaggle/working/gec-t5-base\")","metadata":{"execution":{"iopub.status.busy":"2023-12-22T08:23:50.117382Z","iopub.execute_input":"2023-12-22T08:23:50.117805Z","iopub.status.idle":"2023-12-22T08:23:51.736730Z","shell.execute_reply.started":"2023-12-22T08:23:50.117775Z","shell.execute_reply":"2023-12-22T08:23:51.735524Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"What I did yesterday I don't remember.\n","output_type":"stream"}]},{"cell_type":"code","source":"sentence = \"grammar: One possible outcome are that an environmentally-induced reduction in motorization levels in the richer countrie will outweigh any rise in motorization levels in the poorer countries.\"\ninputs = tokenizer(sentence, return_tensors=\"pt\").input_ids\ngen = corrector.generate(inputs, max_new_tokens=1024)\ncorrected = tokenizer.decode(gen[0], skip_special_tokens=True)\nprint(corrected)","metadata":{"execution":{"iopub.status.busy":"2023-12-22T08:25:45.727119Z","iopub.execute_input":"2023-12-22T08:25:45.727539Z","iopub.status.idle":"2023-12-22T08:25:47.440835Z","shell.execute_reply.started":"2023-12-22T08:25:45.727508Z","shell.execute_reply":"2023-12-22T08:25:47.438578Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"One possible outcome is that an environmentally-induced reduction in motorization levels in the richer countrie will outweigh any rise in motorization levels in the poorer countrie.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### BLEU Evaluation","metadata":{}},{"cell_type":"code","source":"import sacrebleu\nimport csv\nfrom tqdm import tqdm\n\ndata_to_export = []  # List to store data for export\nblue_score = 0\n\nfor i, sentence in tqdm(enumerate(dataset['test']), total=len(dataset['test'])):\n    input_sentence = sentence['input']\n    target = sentence['target']\n    query = \"grammar: \"+input_sentence+\"output:\"\n    input_ids = tokenizer(query, return_tensors=\"pt\").input_ids\n    outputs = corrector.generate(input_ids, max_length=1024)\n    decoder_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    new_blue = sacrebleu.corpus_bleu([decoder_output], [[target]]).score\n\n    blue_score += new_blue\n    data_to_export.append((input_sentence, decoder_output, new_blue))  # Add data to the list\n\nblue_score /= len(dataset['test'])\n\n# # Write data to CSV\n# with open('inference_bleu_scores.csv', 'w', newline='', encoding='utf-8') as csvfile:\n#     writer = csv.writer(csvfile)\n#     writer.writerow(['Input Sentence', 'Inference', 'BLEU Score'])  # Header row\n#     writer.writerows(data_to_export)  # Data rows\n\nprint(\"Average BLEU score:\", blue_score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}