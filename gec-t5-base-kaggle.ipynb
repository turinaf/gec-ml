{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-12-22T06:12:07.796464Z","iopub.status.busy":"2023-12-22T06:12:07.796009Z","iopub.status.idle":"2023-12-22T06:12:08.197745Z","shell.execute_reply":"2023-12-22T06:12:08.196735Z","shell.execute_reply.started":"2023-12-22T06:12:07.796429Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/jfleg-dataset/eval.csv\n","/kaggle/input/jfleg-dataset/train.csv\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-12-22T06:12:08.200218Z","iopub.status.busy":"2023-12-22T06:12:08.199811Z","iopub.status.idle":"2023-12-22T06:12:25.795312Z","shell.execute_reply":"2023-12-22T06:12:25.793980Z","shell.execute_reply.started":"2023-12-22T06:12:08.200192Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"]}],"source":["from transformers import AutoTokenizer\n","from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n","from transformers import DataCollatorForSeq2Seq\n","import datasets\n","import pandas as pd\n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-12-22T06:12:25.797522Z","iopub.status.busy":"2023-12-22T06:12:25.796746Z","iopub.status.idle":"2023-12-22T06:12:32.132172Z","shell.execute_reply":"2023-12-22T06:12:32.131280Z","shell.execute_reply.started":"2023-12-22T06:12:25.797492Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d6714a5248d7472e9f62561ef2a258cd","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"87e04583d17e44a5842dd0a76ec6aaee","version_major":2,"version_minor":0},"text/plain":["spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"65a77c80975148e8a41245c7b6934449","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py:160: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n","For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n","- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n","- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n","- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b2dcaa4b8e2841088eb5e3002f1572d8","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1afb83575b2849c6a917687c690bf5b4","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["checkpoint = \"t5-base\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-12-22T06:12:32.133655Z","iopub.status.busy":"2023-12-22T06:12:32.133378Z","iopub.status.idle":"2023-12-22T06:12:32.277210Z","shell.execute_reply":"2023-12-22T06:12:32.276126Z","shell.execute_reply.started":"2023-12-22T06:12:32.133631Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>input</th>\n","      <th>target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>So I think we can not live if old people could...</td>\n","      <td>So I think we would not be alive if our ancest...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>So I think we can not live if old people could...</td>\n","      <td>So I think we could not live if older people d...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>So I think we can not live if old people could...</td>\n","      <td>So I think we can not live if old people could...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>So I think we can not live if old people could...</td>\n","      <td>So I think we can not live if old people can n...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>For not use car.</td>\n","      <td>Not for use with a car.</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                               input  \\\n","0  So I think we can not live if old people could...   \n","1  So I think we can not live if old people could...   \n","2  So I think we can not live if old people could...   \n","3  So I think we can not live if old people could...   \n","4                                  For not use car.    \n","\n","                                              target  \n","0  So I think we would not be alive if our ancest...  \n","1  So I think we could not live if older people d...  \n","2  So I think we can not live if old people could...  \n","3  So I think we can not live if old people can n...  \n","4                           Not for use with a car.   "]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","\n","train_df = pd.read_csv(\"/kaggle/input/jfleg-dataset/train.csv\")\n","test_def = pd.read_csv(\"/kaggle/input/jfleg-dataset/eval.csv\")\n","\n","combined_df = pd.concat([train_df, test_def], ignore_index=True)\n","combined_df.to_csv(\"/kaggle/working/combined.csv\", index=False)\n","combined_df.head()"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-12-22T06:12:32.280821Z","iopub.status.busy":"2023-12-22T06:12:32.280438Z","iopub.status.idle":"2023-12-22T06:12:32.485084Z","shell.execute_reply":"2023-12-22T06:12:32.484106Z","shell.execute_reply.started":"2023-12-22T06:12:32.280782Z"},"trusted":true},"outputs":[],"source":["import json\n","# Convert combined_df to json format and save it.\n","data = {'data': []}\n","\n","for i in range(len(combined_df)):\n","    json_data = {}\n","    json_data = {\n","        \"input\": \"grammar: \" + combined_df['input'][i],\n","        \"target\": combined_df['target'][i]\n","    }   \n","    data['data'].append(json_data)\n","with open('/kaggle/working/combined.json', 'w') as f:\n","    json.dump(data, f, indent=4, separators=(',', ': ') )"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-12-22T06:12:32.486745Z","iopub.status.busy":"2023-12-22T06:12:32.486364Z","iopub.status.idle":"2023-12-22T06:12:32.720600Z","shell.execute_reply":"2023-12-22T06:12:32.719640Z","shell.execute_reply.started":"2023-12-22T06:12:32.486711Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-ea24147818b0cc8f/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"215ceaea765e47d1ae20d45723d1be42","version_major":2,"version_minor":0},"text/plain":["Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"148411fd30d942289322d4e9b740da7e","version_major":2,"version_minor":0},"text/plain":["Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-ea24147818b0cc8f/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3ec09a4aa4af4bc9b9f4da57aeb8d7f6","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from datasets import load_dataset\n","dataset = load_dataset('json', data_files='/kaggle/working/combined.json',field=\"data\")"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-12-22T06:12:32.722159Z","iopub.status.busy":"2023-12-22T06:12:32.721864Z","iopub.status.idle":"2023-12-22T06:12:32.741684Z","shell.execute_reply":"2023-12-22T06:12:32.740533Z","shell.execute_reply.started":"2023-12-22T06:12:32.722132Z"},"trusted":true},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['input', 'target'],\n","        num_rows: 4803\n","    })\n","    test: Dataset({\n","        features: ['input', 'target'],\n","        num_rows: 1201\n","    })\n","})"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["dataset = dataset['train'].train_test_split(test_size=0.2)\n","dataset"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-12-22T06:12:32.743568Z","iopub.status.busy":"2023-12-22T06:12:32.743193Z","iopub.status.idle":"2023-12-22T06:12:32.750509Z","shell.execute_reply":"2023-12-22T06:12:32.749513Z","shell.execute_reply.started":"2023-12-22T06:12:32.743539Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'input': 'grammar: Or, \" Is this the right thing to do? \" ',\n"," 'target': 'Or, \" Is this the right thing to do? \" '}"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["dataset['train'][0]"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-12-22T06:12:32.752230Z","iopub.status.busy":"2023-12-22T06:12:32.751884Z","iopub.status.idle":"2023-12-22T06:12:32.760045Z","shell.execute_reply":"2023-12-22T06:12:32.759234Z","shell.execute_reply.started":"2023-12-22T06:12:32.752198Z"},"trusted":true},"outputs":[],"source":["def preprocess_func(examples):\n","    inputs = [source for source in examples['input']]\n","    targets = [target for target in examples['target']]\n","    model_inputs = tokenizer(inputs, text_target=targets, max_length=1024, truncation=True)\n","    return model_inputs"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-12-22T06:12:32.761588Z","iopub.status.busy":"2023-12-22T06:12:32.761221Z","iopub.status.idle":"2023-12-22T06:12:33.531996Z","shell.execute_reply":"2023-12-22T06:12:33.530991Z","shell.execute_reply.started":"2023-12-22T06:12:32.761552Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6aeb5454a80946a78559d5a55236315b","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/5 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cfdd331aa56e4eaca6803cfc6c23da7e","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"}],"source":["tokenized_data = dataset.map(preprocess_func, batched=True)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-12-22T06:12:33.534053Z","iopub.status.busy":"2023-12-22T06:12:33.533656Z","iopub.status.idle":"2023-12-22T06:12:33.540793Z","shell.execute_reply":"2023-12-22T06:12:33.539655Z","shell.execute_reply.started":"2023-12-22T06:12:33.534016Z"},"trusted":true},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['input', 'target', 'input_ids', 'attention_mask', 'labels'],\n","        num_rows: 4803\n","    })\n","    test: Dataset({\n","        features: ['input', 'target', 'input_ids', 'attention_mask', 'labels'],\n","        num_rows: 1201\n","    })\n","})"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["tokenized_data"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-12-22T06:12:33.542392Z","iopub.status.busy":"2023-12-22T06:12:33.542051Z","iopub.status.idle":"2023-12-22T06:12:52.531009Z","shell.execute_reply":"2023-12-22T06:12:52.529691Z","shell.execute_reply.started":"2023-12-22T06:12:33.542364Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stdout","output_type":"stream","text":["Collecting evaluate\n","  Obtaining dependency information for evaluate from https://files.pythonhosted.org/packages/70/63/7644a1eb7b0297e585a6adec98ed9e575309bb973c33b394dae66bc35c69/evaluate-0.4.1-py3-none-any.whl.metadata\n","  Downloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n","Collecting sacrebleu\n","  Obtaining dependency information for sacrebleu from https://files.pythonhosted.org/packages/de/ea/025db0a39337b63d4728a900d262c39c3029b0fe76a9876ce6297b1aa6a0/sacrebleu-2.4.0-py3-none-any.whl.metadata\n","  Downloading sacrebleu-2.4.0-py3-none-any.whl.metadata (57 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.4/57.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.1.0)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.24.3)\n","Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.7)\n","Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.0.3)\n","Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.1)\n","Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\n","Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.15)\n","Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2023.12.2)\n","Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.19.4)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\n","Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.18.0)\n","Collecting portalocker (from sacrebleu)\n","  Obtaining dependency information for portalocker from https://files.pythonhosted.org/packages/17/9e/87671efcca80ba6203811540ed1f9c0462c1609d2281d7b7f53cef05da3d/portalocker-2.8.2-py3-none-any.whl.metadata\n","  Downloading portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\n","Requirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (2023.8.8)\n","Requirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\n","Requirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\n","Requirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (4.9.3)\n","Requirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (11.0.0)\n","Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.8.5)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.2)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (6.0.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.0.9)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2023.11.17)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\n","Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n","Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sacrebleu-2.4.0-py3-none-any.whl (106 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n","Installing collected packages: portalocker, sacrebleu, evaluate\n","Successfully installed evaluate-0.4.1 portalocker-2.8.2 sacrebleu-2.4.0\n"]}],"source":["!pip install evaluate sacrebleu"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-12-22T06:12:52.533226Z","iopub.status.busy":"2023-12-22T06:12:52.532841Z","iopub.status.idle":"2023-12-22T06:12:55.093212Z","shell.execute_reply":"2023-12-22T06:12:55.092242Z","shell.execute_reply.started":"2023-12-22T06:12:52.533192Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7d2a720859db4e6d89c5d8deb43445da","version_major":2,"version_minor":0},"text/plain":["Downloading builder script:   0%|          | 0.00/8.15k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["import evaluate\n","metric = evaluate.load(\"sacrebleu\")\n","\n","# Then create a function that passes your predictions and labels to compute to calculate the SacreBLEU score:\n","import numpy as np\n","\n","def postprocess_text(preds, labels):\n","    preds = [pred.strip() for pred in preds]\n","    labels = [[label.strip()] for label in labels]\n","    return preds, labels\n","\n","def compute_metrics(eval_preds):\n","    preds, labels = eval_preds\n","    if isinstance(preds, tuple):\n","        preds = preds[0]\n","    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True, max_new_tokens=1024)\n","#     print(f\"Decoded preds: {decoded_preds} \\n Len = {len(decoded_preds)}\")\n","    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","\n","    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n","\n","    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n","    result = {\"bleu\": result[\"score\"]}\n","\n","    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n","    result[\"gen_len\"] = np.mean(prediction_lens)\n","    result = {k: round(v, 4) for k, v in result.items()}\n","    return result"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-12-22T06:12:55.098887Z","iopub.status.busy":"2023-12-22T06:12:55.097644Z","iopub.status.idle":"2023-12-22T06:12:55.103716Z","shell.execute_reply":"2023-12-22T06:12:55.102534Z","shell.execute_reply.started":"2023-12-22T06:12:55.098852Z"},"trusted":true},"outputs":[],"source":["from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n","\n","data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-12-22T06:12:55.105478Z","iopub.status.busy":"2023-12-22T06:12:55.105098Z","iopub.status.idle":"2023-12-22T06:13:01.743563Z","shell.execute_reply":"2023-12-22T06:13:01.742645Z","shell.execute_reply.started":"2023-12-22T06:12:55.105443Z"},"trusted":true},"outputs":[],"source":["\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir=\"trainer-state\",\n","    evaluation_strategy=\"epoch\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    weight_decay=0.01,\n","    save_total_limit=2,\n","    num_train_epochs=40,\n","    predict_with_generate=True,\n","    fp16=True,\n","    push_to_hub=False,\n","    logging_dir=\"logging\",\n","    logging_strategy=\"epoch\",\n","#     generation_max_length=1024,\n","#     max_new_tokens=1024,  \n",")\n","\n","trainer = Seq2SeqTrainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_data[\"train\"],\n","    eval_dataset=tokenized_data[\"test\"],\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n",")"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-12-22T06:13:01.745119Z","iopub.status.busy":"2023-12-22T06:13:01.744811Z","iopub.status.idle":"2023-12-22T07:40:37.668726Z","shell.execute_reply":"2023-12-22T07:40:37.667491Z","shell.execute_reply.started":"2023-12-22T06:13:01.745089Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":["  ········································\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/html":["Tracking run with wandb version 0.16.1"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20231222_061409-4svijeuk</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/turiabu/huggingface/runs/4svijeuk' target=\"_blank\">dutiful-hill-25</a></strong> to <a href='https://wandb.ai/turiabu/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/turiabu/huggingface' target=\"_blank\">https://wandb.ai/turiabu/huggingface</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/turiabu/huggingface/runs/4svijeuk' target=\"_blank\">https://wandb.ai/turiabu/huggingface/runs/4svijeuk</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='12040' max='12040' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [12040/12040 1:25:54, Epoch 40/40]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Bleu</th>\n","      <th>Gen Len</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.712100</td>\n","      <td>0.501570</td>\n","      <td>50.857200</td>\n","      <td>16.410500</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.553600</td>\n","      <td>0.467730</td>\n","      <td>51.967400</td>\n","      <td>16.394700</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.503900</td>\n","      <td>0.446720</td>\n","      <td>52.628600</td>\n","      <td>16.354700</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.466600</td>\n","      <td>0.435286</td>\n","      <td>52.820100</td>\n","      <td>16.356400</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.435500</td>\n","      <td>0.423996</td>\n","      <td>53.141300</td>\n","      <td>16.333100</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.406700</td>\n","      <td>0.419589</td>\n","      <td>53.291100</td>\n","      <td>16.326400</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.390200</td>\n","      <td>0.416209</td>\n","      <td>53.657200</td>\n","      <td>16.318900</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>0.368600</td>\n","      <td>0.416930</td>\n","      <td>53.667000</td>\n","      <td>16.302200</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>0.353300</td>\n","      <td>0.417876</td>\n","      <td>53.542900</td>\n","      <td>16.308900</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.342100</td>\n","      <td>0.420589</td>\n","      <td>53.400700</td>\n","      <td>16.298900</td>\n","    </tr>\n","    <tr>\n","      <td>11</td>\n","      <td>0.327500</td>\n","      <td>0.423007</td>\n","      <td>53.567600</td>\n","      <td>16.312200</td>\n","    </tr>\n","    <tr>\n","      <td>12</td>\n","      <td>0.317400</td>\n","      <td>0.425065</td>\n","      <td>53.617900</td>\n","      <td>16.298900</td>\n","    </tr>\n","    <tr>\n","      <td>13</td>\n","      <td>0.308400</td>\n","      <td>0.429220</td>\n","      <td>53.530300</td>\n","      <td>16.315600</td>\n","    </tr>\n","    <tr>\n","      <td>14</td>\n","      <td>0.300300</td>\n","      <td>0.433483</td>\n","      <td>53.598800</td>\n","      <td>16.311400</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>0.292300</td>\n","      <td>0.434729</td>\n","      <td>53.516800</td>\n","      <td>16.319700</td>\n","    </tr>\n","    <tr>\n","      <td>16</td>\n","      <td>0.282200</td>\n","      <td>0.437779</td>\n","      <td>53.501300</td>\n","      <td>16.338100</td>\n","    </tr>\n","    <tr>\n","      <td>17</td>\n","      <td>0.277100</td>\n","      <td>0.442073</td>\n","      <td>53.512500</td>\n","      <td>16.336400</td>\n","    </tr>\n","    <tr>\n","      <td>18</td>\n","      <td>0.272900</td>\n","      <td>0.445108</td>\n","      <td>53.416800</td>\n","      <td>16.347200</td>\n","    </tr>\n","    <tr>\n","      <td>19</td>\n","      <td>0.265200</td>\n","      <td>0.448587</td>\n","      <td>53.169100</td>\n","      <td>16.337200</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>0.257900</td>\n","      <td>0.452680</td>\n","      <td>53.172700</td>\n","      <td>16.343000</td>\n","    </tr>\n","    <tr>\n","      <td>21</td>\n","      <td>0.257700</td>\n","      <td>0.454187</td>\n","      <td>53.107500</td>\n","      <td>16.343000</td>\n","    </tr>\n","    <tr>\n","      <td>22</td>\n","      <td>0.253400</td>\n","      <td>0.456963</td>\n","      <td>53.044700</td>\n","      <td>16.338100</td>\n","    </tr>\n","    <tr>\n","      <td>23</td>\n","      <td>0.248500</td>\n","      <td>0.459907</td>\n","      <td>52.952400</td>\n","      <td>16.328900</td>\n","    </tr>\n","    <tr>\n","      <td>24</td>\n","      <td>0.243200</td>\n","      <td>0.464167</td>\n","      <td>52.831400</td>\n","      <td>16.332200</td>\n","    </tr>\n","    <tr>\n","      <td>25</td>\n","      <td>0.244400</td>\n","      <td>0.467642</td>\n","      <td>52.874200</td>\n","      <td>16.333900</td>\n","    </tr>\n","    <tr>\n","      <td>26</td>\n","      <td>0.238100</td>\n","      <td>0.469654</td>\n","      <td>52.425700</td>\n","      <td>16.330600</td>\n","    </tr>\n","    <tr>\n","      <td>27</td>\n","      <td>0.233500</td>\n","      <td>0.475493</td>\n","      <td>52.541400</td>\n","      <td>16.320600</td>\n","    </tr>\n","    <tr>\n","      <td>28</td>\n","      <td>0.235000</td>\n","      <td>0.474475</td>\n","      <td>52.524800</td>\n","      <td>16.326400</td>\n","    </tr>\n","    <tr>\n","      <td>29</td>\n","      <td>0.231200</td>\n","      <td>0.478216</td>\n","      <td>52.496600</td>\n","      <td>16.303100</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.228200</td>\n","      <td>0.480261</td>\n","      <td>52.454000</td>\n","      <td>16.318900</td>\n","    </tr>\n","    <tr>\n","      <td>31</td>\n","      <td>0.225000</td>\n","      <td>0.481951</td>\n","      <td>52.413600</td>\n","      <td>16.316400</td>\n","    </tr>\n","    <tr>\n","      <td>32</td>\n","      <td>0.224100</td>\n","      <td>0.484712</td>\n","      <td>52.398500</td>\n","      <td>16.324700</td>\n","    </tr>\n","    <tr>\n","      <td>33</td>\n","      <td>0.223100</td>\n","      <td>0.485391</td>\n","      <td>52.402100</td>\n","      <td>16.310600</td>\n","    </tr>\n","    <tr>\n","      <td>34</td>\n","      <td>0.221100</td>\n","      <td>0.487913</td>\n","      <td>52.337700</td>\n","      <td>16.311400</td>\n","    </tr>\n","    <tr>\n","      <td>35</td>\n","      <td>0.220000</td>\n","      <td>0.489047</td>\n","      <td>52.251600</td>\n","      <td>16.302200</td>\n","    </tr>\n","    <tr>\n","      <td>36</td>\n","      <td>0.219500</td>\n","      <td>0.488990</td>\n","      <td>52.194900</td>\n","      <td>16.310600</td>\n","    </tr>\n","    <tr>\n","      <td>37</td>\n","      <td>0.217600</td>\n","      <td>0.490085</td>\n","      <td>52.245600</td>\n","      <td>16.310600</td>\n","    </tr>\n","    <tr>\n","      <td>38</td>\n","      <td>0.219700</td>\n","      <td>0.490049</td>\n","      <td>52.157000</td>\n","      <td>16.302200</td>\n","    </tr>\n","    <tr>\n","      <td>39</td>\n","      <td>0.218300</td>\n","      <td>0.490306</td>\n","      <td>52.178400</td>\n","      <td>16.302200</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.216900</td>\n","      <td>0.490585</td>\n","      <td>52.185100</td>\n","      <td>16.303900</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n"]},{"data":{"text/plain":["TrainOutput(global_step=12040, training_loss=0.30130216909009355, metrics={'train_runtime': 5255.5621, 'train_samples_per_second': 36.556, 'train_steps_per_second': 2.291, 'total_flos': 1.172250361930752e+16, 'train_loss': 0.30130216909009355, 'epoch': 40.0})"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["trainer.save_model(\"gec-t5-base\")"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2023-12-22T07:51:52.629287Z","iopub.status.busy":"2023-12-22T07:51:52.628845Z","iopub.status.idle":"2023-12-22T07:52:48.002840Z","shell.execute_reply":"2023-12-22T07:52:48.001625Z","shell.execute_reply.started":"2023-12-22T07:51:52.629236Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stdout","output_type":"stream","text":["updating: kaggle/working/gec-t5-base/ (stored 0%)\n","updating: kaggle/working/gec-t5-base/config.json (deflated 63%)\n","updating: kaggle/working/gec-t5-base/generation_config.json (deflated 29%)\n","updating: kaggle/working/gec-t5-base/tokenizer.json (deflated 74%)\n","updating: kaggle/working/gec-t5-base/training_args.bin (deflated 49%)\n","updating: kaggle/working/gec-t5-base/spiece.model (deflated 48%)\n","updating: kaggle/working/gec-t5-base/tokenizer_config.json (deflated 95%)\n","updating: kaggle/working/gec-t5-base/model.safetensors (deflated 8%)\n","updating: kaggle/working/gec-t5-base/special_tokens_map.json (deflated 86%)\n"]}],"source":["# zip model folder to download\n","!zip -r trained_model1.zip /kaggle/working/gec-t5-base"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2023-12-22T07:53:50.572616Z","iopub.status.busy":"2023-12-22T07:53:50.571623Z","iopub.status.idle":"2023-12-22T07:54:45.914283Z","shell.execute_reply":"2023-12-22T07:54:45.913078Z","shell.execute_reply.started":"2023-12-22T07:53:50.572580Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stdout","output_type":"stream","text":["  adding: kaggle/working/gec-t5-base/ (stored 0%)\n","  adding: kaggle/working/gec-t5-base/config.json (deflated 63%)\n","  adding: kaggle/working/gec-t5-base/generation_config.json (deflated 29%)\n","  adding: kaggle/working/gec-t5-base/tokenizer.json (deflated 74%)\n","  adding: kaggle/working/gec-t5-base/training_args.bin (deflated 49%)\n","  adding: kaggle/working/gec-t5-base/spiece.model (deflated 48%)\n","  adding: kaggle/working/gec-t5-base/tokenizer_config.json (deflated 95%)\n","  adding: kaggle/working/gec-t5-base/model.safetensors (deflated 8%)\n","  adding: kaggle/working/gec-t5-base/special_tokens_map.json (deflated 86%)\n"]}],"source":["# zip model folder to download\n","!zip -r fine-tuned.zip /kaggle/working/gec-t5-base"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2023-12-22T07:56:16.940744Z","iopub.status.busy":"2023-12-22T07:56:16.940310Z","iopub.status.idle":"2023-12-22T07:56:16.952398Z","shell.execute_reply":"2023-12-22T07:56:16.950723Z","shell.execute_reply.started":"2023-12-22T07:56:16.940712Z"},"trusted":true},"outputs":[{"data":{"text/html":["<a href='/kaggle/working/fine-tuned.zip' target='_blank'>/kaggle/working/fine-tuned.zip</a><br>"],"text/plain":["/kaggle/working/fine-tuned.zip"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["from IPython.display import FileLink\n","FileLink('/kaggle/working/fine-tuned.zip')"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2023-12-22T08:04:46.803562Z","iopub.status.busy":"2023-12-22T08:04:46.802205Z","iopub.status.idle":"2023-12-22T08:04:47.918719Z","shell.execute_reply":"2023-12-22T08:04:47.917073Z","shell.execute_reply.started":"2023-12-22T08:04:46.803519Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stdout","output_type":"stream","text":["updating: kaggle/working/trainer-state/checkpoint-12000/ (stored 0%)\n"]}],"source":["# zip trainer-state checkpoint folder to download\n","!zip  trainer_state.zip /kaggle/working/trainer-state/checkpoint-12000"]},{"cell_type":"markdown","metadata":{},"source":["### Inference"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2023-12-22T08:23:50.117805Z","iopub.status.busy":"2023-12-22T08:23:50.117382Z","iopub.status.idle":"2023-12-22T08:23:51.736730Z","shell.execute_reply":"2023-12-22T08:23:51.735524Z","shell.execute_reply.started":"2023-12-22T08:23:50.117775Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["What I did yesterday I don't remember.\n"]}],"source":["tokenizer = AutoTokenizer.from_pretrained(\"/kaggle/working/gec-t5-base\", model_max_length=1024)\n","corrector = AutoModelForSeq2SeqLM.from_pretrained(\"/kaggle/working/gec-t5-base\")"]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2023-12-22T08:25:45.727539Z","iopub.status.busy":"2023-12-22T08:25:45.727119Z","iopub.status.idle":"2023-12-22T08:25:47.440835Z","shell.execute_reply":"2023-12-22T08:25:47.438578Z","shell.execute_reply.started":"2023-12-22T08:25:45.727508Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["One possible outcome is that an environmentally-induced reduction in motorization levels in the richer countrie will outweigh any rise in motorization levels in the poorer countrie.\n"]}],"source":["sentence = \"grammar: One possible outcome are that an environmentally-induced reduction in motorization levels in the richer countrie will outweigh any rise in motorization levels in the poorer countries.\"\n","inputs = tokenizer(sentence, return_tensors=\"pt\").input_ids\n","gen = corrector.generate(inputs, max_new_tokens=1024)\n","corrected = tokenizer.decode(gen[0], skip_special_tokens=True)\n","print(corrected)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["### BLEU Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import sacrebleu\n","import csv\n","from tqdm import tqdm\n","\n","data_to_export = []  # List to store data for export\n","blue_score = 0\n","\n","for i, sentence in tqdm(enumerate(dataset['test']), total=len(dataset['test'])):\n","    input_sentence = sentence['input']\n","    target = sentence['target']\n","    query = \"grammar: \"+input_sentence+\"output:\"\n","    input_ids = tokenizer(query, return_tensors=\"pt\").input_ids\n","    outputs = corrector.generate(input_ids, max_length=1024)\n","    decoder_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","    \n","    new_blue = sacrebleu.corpus_bleu([decoder_output], [[target]]).score\n","\n","    blue_score += new_blue\n","    data_to_export.append((input_sentence, decoder_output, new_blue))  # Add data to the list\n","\n","blue_score /= len(dataset['test'])\n","\n","# # Write data to CSV\n","# with open('inference_bleu_scores.csv', 'w', newline='', encoding='utf-8') as csvfile:\n","#     writer = csv.writer(csvfile)\n","#     writer.writerow(['Input Sentence', 'Inference', 'BLEU Score'])  # Header row\n","#     writer.writerows(data_to_export)  # Data rows\n","\n","print(\"Average BLEU score:\", blue_score)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4015274,"sourceId":6986595,"sourceType":"datasetVersion"}],"dockerImageVersionId":30626,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
