{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from evaluate import load\n",
    "# gleu_metric = load('glue', 'mrpc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.gleu_score import corpus_gleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "eval_dataset = load_dataset(\"csv\", data_files=[\"data/eval_data.csv\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input', 'target'],\n",
       "        num_rows: 1201\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "checkpoint = \"model/gec-t5-base-40ep\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_func(examples):\n",
    "    inputs = [source for source in examples['input']]\n",
    "    targets = [target for target in examples['target']]\n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=1024, truncation=True)\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05ae34cccd0147d787e2710103128f91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1201 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_data = eval_dataset.map(preprocess_func, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input', 'target', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1201\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data['train']['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(eval_dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "corrector = pipeline(\"text2text-generation\", model=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1201/1201 [16:34<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLEU score:  0.45690885316021235\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "references = []\n",
    "predictions = []\n",
    "for i , data in tqdm(enumerate(eval_dataset['train']), total=len(eval_dataset['train'])):\n",
    "    input_s = eval_dataset['train'][i]['input']\n",
    "    # print(\"Input: \", input_s)\n",
    "    input_ids = tokenizer(input_s, return_tensors=\"pt\").input_ids\n",
    "    gen = model.generate(input_ids, max_new_tokens=1024)\n",
    "    pred = tokenizer.decode(gen[0], skip_special_tokens=True)\n",
    "    target = eval_dataset['train'][i]['target']\n",
    "    # print(\"Target: \", target)\n",
    "    # correction = corrector(input_s)\n",
    "    # pred = correction[0]['generated_text']\n",
    "    references.append(target)\n",
    "    predictions.append(pred)\n",
    "\n",
    "gleu_score = corpus_gleu([[ref] for ref in references], [pred for pred in predictions], max_len=1024)\n",
    "print(f\"GLEU score: \", gleu_score)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictions =[]\n",
    "references = []\n",
    "model.eval()\n",
    "input_ids = tokenized_data['train']['input_ids']\n",
    "target_ids = tokenized_data['train']['labels']\n",
    "outputs = model.generate(input_ids=input_ids, max_length=1024)\n",
    "\n",
    "predictions.extend(tokenizer.batch_decode(output, skip_special_tokens=True) for output in outputs)\n",
    "references.extend(tokenizer.decode(target_ids[i], skip_special_tokens=True) for i in range(len(target_ids)))\n",
    "        \n",
    "gleu_score = corpus_gleu([[ref] for ref in references], [pred for pred in predictions])\n",
    "\n",
    "print(f\"GLEU Score: {gleu_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(8):\n",
    "    test_loss = []\n",
    "    predictions =[]\n",
    "    references = []\n",
    "    model.eval()\n",
    "    for batch in tokenized_data:\n",
    "        input_ids = batch['train']['input_ids']\n",
    "        target_ids = batch['train']['labels']\n",
    "        outputs = model.generate(input_ids=input_ids, max_length=1024)\n",
    "        \n",
    "        predictions.extend(tokenizer.decode(output, skip_special_tokens=True) for output in outputs)\n",
    "        references.extend(tokenizer.decode(target_ids[i], skip_special_tokens=True) for i in range(len(target_ids)))\n",
    "        \n",
    "    gleu_score = corpus_gleu([[ref] for ref in references], [pred for pred in predictions])\n",
    "    print(f\"Epoch: {epoch+1} GLUE: {gleu_score}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then create a function that passes your predictions and labels to compute to calculate the SacreBLEU score:\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
